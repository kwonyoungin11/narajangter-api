# 나라장터 API 성능 최적화 가이드

## 현재 성능 현황

### API 연결 테스트 결과
- **입찰공고정보 API**: ✅ 성공 (전체 31,691건 조회 가능)
- **낙찰정보 API**: ⚠️ 응답 형식 문제
- **계약정보 API**: ❌ 타임아웃 (10초)

### 응답 시간
- 입찰공고 조회 (10건): ~2-3초
- 낙찰정보 조회: 타임아웃 발생
- 데이터베이스 쿼리: < 100ms

## 성능 병목 지점

### 1. API 호출 제약사항
```
- 입찰공고: 1개월 범위 제한 (최대 31일)
- 낙찰정보: 1주일 범위 제한 (최대 7일)  
- 계약정보: 1개월 범위 제한
- 일일 호출 제한: 개발계정 10,000회/일
```

### 2. 대용량 데이터 처리
- 2025년 1월 기준 입찰공고 31,691건
- 페이지당 최대 100건 제한
- 전체 동기화 시 최소 317회 API 호출 필요

## 성능 최적화 전략

### 1. 캐싱 전략
```python
# 데이터베이스 캐싱
- 중복 데이터 체크 후 저장
- 기존 데이터 업데이트 최소화
- 인덱스 활용: bid_notice_no, bid_notice_ord

# API 응답 캐싱
- 자주 조회되는 데이터 로컬 저장
- 실시간성이 낮은 데이터 캐싱 (분석 데이터)
```

### 2. 배치 처리 최적화
```python
# 현재 구현 (비효율적)
for item in items:
    existing = BidNotice.query.filter_by(...).first()
    if not existing:
        db.session.add(bid_notice)
db.session.commit()

# 개선안 (bulk insert)
new_items = []
existing_nos = set(db.session.query(BidNotice.bid_notice_no).all())
for item in items:
    if item['bidNtceNo'] not in existing_nos:
        new_items.append(BidNotice(...))
db.session.bulk_insert_mappings(BidNotice, new_items)
db.session.commit()
```

### 3. 페이지네이션 최적화
```python
# 병렬 처리를 위한 페이지 분할
import concurrent.futures

def fetch_page(page_no):
    params = {
        'pageNo': page_no,
        'numOfRows': 100,
        # ... 기타 파라미터
    }
    return requests.get(url, params=params)

# 병렬 페이지 조회
with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
    futures = [executor.submit(fetch_page, i) for i in range(1, 6)]
    results = [f.result() for f in futures]
```

### 4. 데이터베이스 인덱스 최적화
```sql
-- 자주 조회되는 컬럼에 인덱스 추가
CREATE INDEX idx_bid_notice_no ON bid_notices(bid_notice_no);
CREATE INDEX idx_rgst_dt ON bid_notices(rgst_dt);
CREATE INDEX idx_dminstt_nm ON bid_notices(dminstt_nm);
CREATE INDEX idx_work_div_nm ON bid_notices(work_div_nm);

-- 복합 인덱스
CREATE INDEX idx_bid_notice_composite ON bid_notices(bid_notice_no, bid_notice_ord);
```

## 실시간 모니터링

### 1. API 호출 모니터링
```python
import time
import logging

def api_call_with_monitoring(url, params):
    start_time = time.time()
    try:
        response = requests.get(url, params=params, timeout=10)
        elapsed_time = time.time() - start_time
        
        logging.info(f"API Call: {url}")
        logging.info(f"Response Time: {elapsed_time:.2f}s")
        logging.info(f"Status Code: {response.status_code}")
        logging.info(f"Data Size: {len(response.content)} bytes")
        
        return response
    except Exception as e:
        logging.error(f"API Call Failed: {e}")
        raise
```

### 2. 데이터베이스 쿼리 모니터링
```python
# Flask-SQLAlchemy 쿼리 로깅
app.config['SQLALCHEMY_ECHO'] = True  # 개발 환경에서만

# 쿼리 실행 시간 측정
from sqlalchemy import event
from sqlalchemy.engine import Engine
import time

@event.listens_for(Engine, "before_cursor_execute")
def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    conn.info.setdefault('query_start_time', []).append(time.time())

@event.listens_for(Engine, "after_cursor_execute")
def after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    total = time.time() - conn.info['query_start_time'].pop(-1)
    if total > 0.5:  # 0.5초 이상 걸리는 쿼리 로깅
        print(f"Slow Query ({total:.2f}s): {statement[:100]}")
```

## 부하 테스트

### 1. 단위 테스트
```bash
# API 엔드포인트 테스트
curl -w "@curl-format.txt" -o /dev/null -s http://localhost:5000/api/narajangter/bid-notices

# curl-format.txt 내용
time_namelookup:  %{time_namelookup}s\n
time_connect:  %{time_connect}s\n
time_appconnect:  %{time_appconnect}s\n
time_pretransfer:  %{time_pretransfer}s\n
time_redirect:  %{time_redirect}s\n
time_starttransfer:  %{time_starttransfer}s\n
time_total:  %{time_total}s\n
```

### 2. 부하 테스트 (Apache Bench)
```bash
# 동시 접속 10, 총 100회 요청
ab -n 100 -c 10 http://localhost:5000/api/narajangter/bid-notices

# POST 요청 테스트
ab -n 100 -c 10 -p post_data.json -T application/json http://localhost:5000/api/narajangter/sync-bid-notices
```

## 권장 성능 목표

### 응답 시간 목표
- API 목록 조회: < 1초
- 데이터 동기화 (100건): < 5초
- 분석 데이터 조회: < 2초
- 웹 페이지 로딩: < 3초

### 처리량 목표
- 동시 사용자: 50명
- 초당 요청 처리: 100 TPS
- 일일 데이터 동기화: 10,000건

## 성능 개선 로드맵

### Phase 1: 즉시 적용 가능 (1주)
- [x] API 엔드포인트 최신 버전으로 업데이트
- [ ] 데이터베이스 인덱스 추가
- [ ] API 타임아웃 설정 조정 (10초 → 30초)
- [ ] 에러 핸들링 개선

### Phase 2: 단기 개선 (2-4주)
- [ ] Redis 캐싱 도입
- [ ] 배치 처리 최적화
- [ ] 비동기 처리 도입 (Celery)
- [ ] 페이지네이션 개선

### Phase 3: 장기 개선 (1-2개월)
- [ ] 마이크로서비스 아키텍처 전환
- [ ] GraphQL API 도입
- [ ] 실시간 데이터 스트리밍
- [ ] 분산 처리 시스템 구축

## 트러블슈팅

### 일반적인 성능 문제 해결

1. **API 타임아웃**
   - 타임아웃 시간 증가
   - 요청 데이터 범위 축소
   - 재시도 로직 구현

2. **메모리 부족**
   - 페이지네이션 활용
   - 스트리밍 처리
   - 불필요한 데이터 제거

3. **데이터베이스 병목**
   - 쿼리 최적화
   - 인덱스 추가
   - 커넥션 풀 조정

4. **동시성 문제**
   - 트랜잭션 격리 수준 조정
   - 락 최소화
   - 비동기 처리 도입

## 모니터링 도구 추천

1. **APM (Application Performance Monitoring)**
   - New Relic
   - DataDog
   - Elastic APM

2. **로깅 및 분석**
   - ELK Stack (Elasticsearch, Logstash, Kibana)
   - Grafana + Prometheus
   - Sentry (에러 트래킹)

3. **데이터베이스 모니터링**
   - pgAdmin (PostgreSQL 전환 시)
   - MySQL Workbench (MySQL 전환 시)
   - SQLite Browser (현재)